{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML and web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "- Advanced Business Analytics course by Stanislav Borysov, DTU Management.\n",
    "- https://www.w3schools.com/html/default.asp\n",
    "- https://towardsdatascience.com/5-strategies-to-write-unblock-able-web-scrapers-in-python-5e40c147bdaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web crawling and scraping represent a very flexible way to get the content from the Internet. Essentially, it imitates a user who visits different webpages and views their content. The only difference is that Internet companies usually love real users and hate scraping bots. So be prepared to be blocked. **In the worst case, you can get into serious troubles so please always read Terms & Conditions and follow the company's policy about automatic data collection (or ask them directly if you are not sure)!**\n",
    "\n",
    "Additionally, you can always read the **Robot.txt** file to know what you are allowed or not to do. In a nutshel **Robots.txt** is a text file webmasters create to instruct web robots (typically search engine robots) how to crawl pages on their website. In practice, robots.txt files indicate whether certain user agents (web-crawling software) can or cannot crawl parts of a website. These crawl instructions are specified by “disallowing” or “allowing” the behavior of certain (or all) user agents. The basic format is:\n",
    "\n",
    "User-agent: [user-agent name] \n",
    "\n",
    "Disallow: [URL string not to be crawled]\n",
    "\n",
    "\n",
    "It follows an example with comments of the Robot.txt file of Buzzfeed (reference: https://moz.com/learn/seo/robotstxt )\n",
    "\n",
    "![Robot.txt](https://moz-static.s3.amazonaws.com/learn/seo/Robots.txt-and-Robots-meta-directives/_large/Robots.txt.png?mtime=20170427090304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HTML\n",
    "\n",
    "Hypertext Markup Language (HTML) is the standard markup language for documents designed to be displayed in a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Basic structure\n",
    "\n",
    "- All HTML documents must start with a document type declaration: $<!DOCTYPE html>$.\n",
    "- The HTML document itself begins with $<html>$ and ends with $</html>$.\n",
    "- The visible part of the HTML document is between $<body>$ and $</body>$.\n",
    "\n",
    "Let's see an example ( _%%HTML_ is only needed to run HTML code in Jupyter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML  \n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h3>My First Heading</h3>\n",
    "<p>My first paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Elements Tags\n",
    "\n",
    "An HTML element usually consists of a start tag and an end tag, with the content inserted in between. The HTML element is everything from the start tag to the end tag. \n",
    "\n",
    "HTML tags are the hidden keywords within a web page that define how your web browser must format and display the content. Most tags must have two parts, an opening and a closing part. For example, <html> is the opening tag and </html> is the closing tag. Note that the closing tag has the same text as the opening tag, but has an additional forward-slash ( / ) character. Ii can be interpreteted as the \"end\" or \"close\" character.\n",
    "\n",
    "Common tags are:\n",
    "- __Headings__  defined with $<h >$ and a number from 1 to 6. $<h1>$ defines the most important heading, $<h6>$ defines the least important.\n",
    "- __Paragraphs__  defined with the $<p>$ tag.\n",
    "- __Links__  defined with the $<a>$ tag. Where the link's destination is specified in the $href$ attribute.\n",
    "- __Images__  defined with the $<img>$ tag. Where the source file $(src)$, alternative text $(alt)$, $width$, and $height$ are provided as attributes.\n",
    "- __Buttons__  defined with the $<button>$ tag.\n",
    "- __Lists__  defined with the $<ul>$ (unordered/bullet list) or the $<ol>$ (ordered/numbered list) tag, followed by $<li>$ tags (list items).\n",
    "\n",
    "List of Elements Tag: https://www.w3schools.com/tags/ref_byfunc.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Elements container\n",
    "\n",
    "As a \"pure\" container, the $<div>$ or $<span>$ element does not inherently represent anything. Instead, it's used to group content so it can be easily styled using the class or id attributes, marking a section of a document as being written in a different language (using the lang attribute), and so on.\n",
    "\n",
    "Main container elements:\n",
    "- The __$<div>$ element__ is often used as a container for other HTML elements.\n",
    "- The __$<span>$ element__ is often used as a container for some text.\n",
    "\n",
    "Keep in mind that HTML elements can be nested (elements can contain elements). For example, the <body> element of an HTML document can contain paragraph and headings. (See the first example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Elements Attributes\n",
    "\n",
    "Attributes provide additional information about an element. Attributes are always specified in the start tag. For example,\n",
    "- $href$ specifies the link address.\n",
    "- $src$ specifies filename of the image source.\n",
    "- $width$ and $height$ specifies the width and height of the image.\n",
    "- $alt$ attribute specifies an alternative text to be used, if an image cannot be displayed.\n",
    "- $style$ is used to specify the styling of an element, like color, font, size etc.\n",
    "- The language is declared with the $lang$ attribute.\n",
    "- $title$ attribute is added to the $<p>$ element. The value of the title attribute will be displayed as a tooltip when you mouse over the paragraph.\n",
    "- $class$ attribute is used to define equal styles for elements with the same class name.\n",
    "- $id$ attribute specifies a unique id for an HTML element (the value must be unique within the HTML document).\n",
    "\n",
    "Complete list of attributes: https://www.w3schools.com/tags/ref_attributes.asp\n",
    "\n",
    "To get a better idea look at the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en-US\">\n",
    "<div style=\"background-image: url('img_girl.jpg');\">\n",
    "<body>\n",
    "\n",
    "<a href=\"https://www.w3schools.com\">This is a link</a>\n",
    "\n",
    "<img src=\"img_girl.jpg\" width=\"50\" height=\"60\" alt=\"Girl with a jacket\">\n",
    "\n",
    "<p style=\"color:red; font-family:courier;font-size:160%;\">This is the first paragraph.</p>\n",
    "\n",
    "<p title=\"It really happens\">\n",
    "This is the second paragraph.\n",
    "</p>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<style>\n",
    ".cities {\n",
    "  background-color: black;\n",
    "  color: white;\n",
    "  margin: 20px;\n",
    "  padding: 20px;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<div class=\"cities\">\n",
    "  <h2>London</h2>\n",
    "  <p>London is the capital of England.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Tables\n",
    "\n",
    "An HTML table is defined with the $<table>$ tag.\n",
    "\n",
    "Each table row is defined with the $<tr>$ tag. A table header is defined with the $<th>$ tag. By default, table headings are bold and centered. A table data/cell is defined with the $<td>$ tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Firstname</th>\n",
    "    <th>Lastname</th>\n",
    "    <th>Age</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Jill</td>\n",
    "    <td>Smith</td>\n",
    "    <td>50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Eve</td>\n",
    "    <td>Jackson</td>\n",
    "    <td>94</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Web Scraping \n",
    "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 DTU News\n",
    "\n",
    "In this exercise, we will implement a web scraper that get news headlines together with the date and short description from https://www.dtu.dk/english/news. We are gonna use Scrapy (https://scrapy.org/) however, keep in mind that exist many other framework available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Navigating the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the webpage shows 10 recent news items by default; however, it is possible to get the 100 most recent news modifying the URL structure properly. \n",
    "\n",
    "It is very important to get familiar with the website where we are working. We need to know what can and cannot be obtained and we need to understand the URL structure.\n",
    "\n",
    "In the example below we want the 200 most recent. Since only the maximum of news that can be displayed is 100, we construct 2 urls: the first the get the 100 most recent news and the second to get from the 101st to the 200th most recent news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://www.dtu.dk/english/news\"\n",
    "\n",
    "import math\n",
    "\n",
    "urls = []\n",
    "n_items = 200\n",
    "start = 1\n",
    "items_to_show = 100 # The webpage cannot show more then 100 items at once\n",
    "for i in range(math.ceil(n_items / items_to_show)):\n",
    "    parameters = [\n",
    "        \"fr={}\".format(start), \n",
    "        \"mr={}\".format(items_to_show)\n",
    "    ]\n",
    "    url = url_base + \"?\" + \"&\".join(parameters)\n",
    "    urls.append(url)\n",
    "    start += items_to_show\n",
    "\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Scraping the webpage content\n",
    "\n",
    "Let's retrieve the information we are interested in and save it in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "for url in urls:\n",
    "    contents = urllib.request.urlopen(url).read()\n",
    "    with open(\"dtu_news.html\", \"wb\") as f:\n",
    "        f.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that all markup and images are missing since we saved only the HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Extracting the data\n",
    "\n",
    "Let's load the data from the file. Once the web content is loaded, we want to extract information wanted. To do that it can be used regular expressions or HTML parser (e.g., PyQuery, BeautifulSoup) that allows to save the content to a JSON file.\n",
    "\n",
    "To extract any kind of information from a HTML document is very important to understand the structure of the webpage. This can be done directly from your browser. Try:\n",
    "- Chrome: right click -> inspect\n",
    "- Safari, Firefox, Edge: right click -> inspect element\n",
    "\n",
    "Once we have clear the webpage structure, we create a PyQuery object that store all the element we are interested in. To do that we find the information we need inspecting the webpage in our browser and we specify an attribute that identify each single element. Note that often the information are stored in containers and has a particular class. Then, we can iterate through the list of elements and retrieve the needed information specify the attribute name or $.text()$ if the information is stored as text. If the the information is stored in a sub-element, we can use the function $find$. To access sub element we can also use the \">\" symbol (this depends on the web-scraper framework (PyQuery) and can be different for other framework). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"dtu_news.html\", \"r\", encoding='utf8') as f:\n",
    "#     contents = f.read()\n",
    "    \n",
    "from pyquery import PyQuery \n",
    "\n",
    "# Let's take the first 100 most recent news\n",
    "\n",
    "news = []\n",
    "for url in urls:\n",
    "    # Request the content of the website\n",
    "    contents = urllib.request.urlopen(url).read()\n",
    "\n",
    "    # load the document\n",
    "    pq = PyQuery(contents)\n",
    "\n",
    "    # Get all the element that has 'div.newsItem' as tag.\n",
    "    # We know that from inspecting the webpage. \n",
    "    newsItems = pq('div.newsItem')\n",
    "    \n",
    "    #\n",
    "    for i in range(len(newsItems)):\n",
    "        # one news\n",
    "        item = newsItems.eq(i)\n",
    "        # url\n",
    "        item_url = item.find('h2>a').attr('href')\n",
    "        # title\n",
    "        item_title = item.find('h2>a').text()\n",
    "        # desc\n",
    "        item_desc = item.find('p').text()\n",
    "        # date\n",
    "        item_date = item.find('span.date').text()\n",
    "        # \n",
    "        new_item = {\n",
    "            'url': item_url,\n",
    "            'title': item_title,\n",
    "            'desc': item_desc,\n",
    "            'date': item_date\n",
    "        }\n",
    "        \n",
    "        print(new_item)\n",
    "        news.append(new_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pictures links MLSM\n",
    "\n",
    "In this example, we want to get the picture link of the people of the MLSM group (http://mlsm.man.dtu.dk). \n",
    "\n",
    "Firstly, we need to specify the webpage url and request the page content.\n",
    "\n",
    "Second, we load the document in Scrapy\n",
    "\n",
    "Inspecting the page, we find that the picture link is stored as attribute $scr$ in image ($img$) elements which class is \".person-img.circle\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify the url\n",
    "url = \"http://mlsm.man.dtu.dk/people/\"\n",
    "# get the content\n",
    "contents = urllib.request.urlopen(url).read()   \n",
    "# load the document in PyQuery\n",
    "pq = PyQuery(contents)\n",
    "# Get all the elements with \"img\" tag inside the class \"person-img.circle\"\n",
    "newsItems = pq('.person-img.circle > img')\n",
    "# for each element print the source link.\n",
    "for x in newsItems.items():\n",
    "    print(x.attr('src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the today temperature from \n",
    "url = \"https://www.foreca.com/Denmark/Kongens_Lyngby\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import urllib.request\n",
    "\n",
    "    url = \"https://www.foreca.com/Denmark/Kongens_Lyngby\"\n",
    "    contents = urllib.request.urlopen(url).read()\n",
    "    pq = PyQuery(contents)\n",
    "\n",
    "\n",
    "    # Here we can see multiple ways to get the same element \n",
    "\n",
    "    x = pq(\"div.left >span.warm.txt-xxlarge \")\n",
    "    print(x.text())\n",
    "\n",
    "    newsItems = pq(\"div.left\")\n",
    "    x=newsItems.find(\"span.warm.txt-xxlarge\")\n",
    "    print(x.text())\n",
    "\n",
    "    newsItems = pq(\"span.warm.txt-xxlarge\")\n",
    "    print(newsItems.text())\n",
    "\n",
    "    newsItems = pq(\".warm.txt-xxlarge\")\n",
    "    print(newsItems.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to not get blacklisted\n",
    "\n",
    "When scraping it is possible to be blocked because website owner do not like much scrapers. Here there are some strategies to not be blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 User agent\n",
    "\n",
    "One thing is to set a user-agent. User Agent is a tool that works on behalf of the user and tells the server about which web browser the user is using for visiting the website. Many websites do not let you view the content if the user-agent is not set.\n",
    "This can be easily done using the **request** package that works exatcly as **urllib.request**\n",
    "\n",
    "Try first to request the HTML page from H&M strore webpage first in the way we have seen before and, then, using User agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www2.hm.com/da_dk/herre/produkt/hoodies-og-sweatshirts.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Non-User Agent\n",
    "try:\n",
    "    cntents = urllib.request.urlopen(url).read()\n",
    "except:\n",
    "    print(\"HTTP Error 403: Forbidden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "    }\n",
    "\n",
    "# User Agent\n",
    "r = requests.get(url,headers=headers)\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Delays\n",
    "\n",
    "When you want to make several request to the same website, a good way to mimic the human behaviour is to wait between requests. You can do that using time.sleep() and wait for a random number of seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "delay=np.random.randint(0,10)\n",
    "time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Other ways to not get blocked\n",
    "\n",
    "It exist many other trick to not get blocked. A general suggestion is to immitate as much as possible human behaviour:\n",
    "- Delays your requests;\n",
    "- Limit the amount of data downloaded at once;\n",
    "- Do not follow the same crawling pattern.\n",
    "\n",
    "It is also possible to make multiple requests using different IP, which somehow means pretenting to be different user. The sites can figure out the pattern of a certain IP or a pool of IP and simply block them. But, it is possible to buy IPs and use them to make requests. Similarly, you can rotate user agent. To do that you can use list of random User agent such as [Udger](https://udger.com/resources/ua-list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

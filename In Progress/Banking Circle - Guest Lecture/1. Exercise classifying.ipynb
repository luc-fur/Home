{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "The exercise will attempt to convey important skills and concepts in applying data science in a production environment.\n",
    "- Thinking about the data\n",
    " - what is it like and what could I do with it?\n",
    " - what is the business objective?\n",
    "- Generating value from ML:\n",
    " - establishing a baseline\n",
    " - improving as required\n",
    " \n",
    "\n",
    "## The challange that you have been set:\n",
    "You are part of a fast growing social media startup HypeVentures that provides chat and discussion space technology to other startup to improve the customer engagement with the content the users post. Management has hired you to sort out their marketing messaging as the last marketing guy quit and they intend to hire a bunch of different topic experts to deal with the different forums.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what do people talk about?\n",
    " 1. Can we cluster the conversation topics?\n",
    " 2. Can we label some of them by hand?\n",
    " 3. Use that to label the rest?\n",
    "- Classify some new incomming data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons learnt\n",
    "- Humble pie\n",
    " - unlabled data is hard (unsupervised learning)\n",
    " - human labeling is extremely valuable\n",
    " - Always check your work\n",
    "- Curse of dimensionality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- get the data labeled [Amazon SageMaker](https://aws.amazon.com/sagemaker/groundtruth/pricing/)\n",
    "- Can we train a classifer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports we'll need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from exercise_utils import plot_confusion_matrix, get_bank_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklean features\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ingress\n",
    "load 'data_comp.graphics_sci.med_categories_train.csv' into a pandas dataframe and inspect it\n",
    "load 'data_comp.graphics_sci.med_categories_test.csv' into a pandas dataframe and inspect it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data_comp.graphics_sci.med_categories_train.csv').dropna()\n",
    "df2_test = pd.read_csv('data_comp.graphics_sci.med_categories_test.csv').dropna()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vecotrize the text data \n",
    "\n",
    "1. Either using a counting vectorizer \n",
    "2. or a hashing one\n",
    "\n",
    "Using english default stop words:\n",
    "stop words are words like: and, to, I etc. Complete list for a common usecase: https://gist.github.com/sebleier/554280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorization again (with stopwords this time)\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(df2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data as before (SVD and normalization)\n",
    "svd = TruncatedSVD(n_components=40)\n",
    "X_reduced = svd.fit_transform(X_train_counts)\n",
    "\n",
    "norm = Normalizer()\n",
    "X = norm.fit_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need non negative values, so add the (absolute) minimum to all\n",
    "X += abs(X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit a linear model (SGDClassifier with hinge loss)\n",
    "# Essentialy and SVM with stochastic gradient descent\n",
    "# more info here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "clfb = SGDClassifier(loss='hinge',tol=None,random_state=0, max_iter=5).fit(X, df2.subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the model we have specifiend\n",
    "clfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the targets \n",
    "targets = df2.groupby('subject')['category'].first().sort_index().tolist()\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make up some sentences and see what it predicts\n",
    "docs_new = [\n",
    "    \"My new Intel CPU is great\",\n",
    "    \"I have a really bad cold\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the sentences and predict\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = svd.transform(X_new_counts)\n",
    "predicted = clfb.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predictions in a pretty way\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print(f'{doc} => {targets[category]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wrap the classifier in a sklearn Pipeline, so we don't need to redo all the steps\n",
    "# Let's also try and use a different model, say a Naive Bayes approach\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline\n",
    "text_clf.fit(df2.data, df2.subject)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the pipeline\n",
    "text_clf.score(df2_test.data, df2_test.subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same for our SDG Classifier\n",
    "text_clf = Pipeline([\n",
    "   ('vect', CountVectorizer(stop_words='english')),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                         max_iter=50, tol=None)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(df2.data, df2.subject)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.score(df2_test.data, df2_test.subject) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(df2_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classificalt report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(df2_test.subject, predicted,\n",
    "    target_names=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using plot_confusion_matrix from the helper module, plot the matrix to inspect it\n",
    "cm = confusion_matrix(df2_test.subject, predicted)\n",
    "plot_confusion_matrix(df2_test.subject, predicted,targets,\n",
    "                          normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus material\n",
    "With more difficult data or more and different data.\n",
    "load one of the following: \n",
    "\n",
    "\n",
    "\n",
    "data_all_categories_train and test\n",
    "\n",
    "data_alt.atheism_soc.religion.christian_categories_train and test\n",
    "\n",
    "data_comp.graphics_sci.med_alt.atheism_soc.religion.christian_categories train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = lambda x: f'data_all_categories_{x}.csv'\n",
    "#file = lambda x: f'data_alt.atheism_soc.religion.christian_categories_{x}.csv'\n",
    "#file = lambda x: f'data_comp.graphics_sci.med_alt.atheism_soc.religion.christian_categories_{x}.csv'\n",
    "df = pd.read_csv(file('train')).dropna()\n",
    "df_test = pd.read_csv(file('test')).dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.data\n",
    "y_train = df.subject\n",
    "\n",
    "targets = df.groupby('subject')['category'].first().sort_index().tolist()\n",
    "X_test = df_test.data\n",
    "y_test = df_test.subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(X_train, y_train)  \n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred,\n",
    "    target_names=targets))\n",
    "plot_confusion_matrix(y_test, y_pred,targets,\n",
    "                          normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More bonus material\n",
    "\n",
    "Real complaints about banks in America\n",
    "\n",
    "load the data from bank_data.csv or use get_bank_data, only use the top 10_000 entries, unless you have a very powerfull PC or lot's of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bank_data.csv\").head(20_000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's grab the targets again\n",
    "targets = df.groupby('subject')['category'].first().sort_index().tolist()\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the train_test_split from sklearn to make a train and a test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.data, df.subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and plot the confusion matrix\n",
    "text_clf.fit(X_train, y_train)  \n",
    "y_pred = text_clf.predict(X_test)\n",
    "plot_confusion_matrix(y_test, y_pred,targets,\n",
    "                          normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's add some better stopwords to see if we can improve things\n",
    "from sklearn.feature_extraction import text \n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['XX','XXX', 'XXXX'])\n",
    "text_clf.set_params(vect__stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again we fit and evaluate\n",
    "text_clf.fit(X_train, y_train)  \n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred,targets,\n",
    "                          normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at more metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print(f'Testing accuracy {accuracy_score(y_test, y_pred):.2%}')\n",
    "print(f\"Testing F1 score: {f1_score(y_test, y_pred, average='weighted'):.2%}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus bonus material\n",
    "For doc2vec / gensim approach to the same bank data checkout\n",
    "https://github.com/susanli2016/NLP-with-Python/blob/master/Doc2Vec%20Consumer%20Complaint_3.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
